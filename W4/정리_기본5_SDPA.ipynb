{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8527e6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c577ff02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31e413e",
   "metadata": {},
   "source": [
    "## Scaled Dot-Product Attention\n",
    "\n",
    "- $X \\in \\mathbb{R}^{n \\times d}$\n",
    "    - $n$: number of data = sequence length = number of words\n",
    "    - $d$: propagating vector = dimension of word embedding vector = usually a size of 512\n",
    "    - Complexity per Layer of Self-Attention: $O(n^2\\cdot d)$\n",
    "- $Q, K \\in \\mathbb{R}^{n \\times d_K}$ \n",
    "- $V \\in \\mathbb{R}^{n \\times d_V} $\n",
    "\n",
    "$\\text{Attention}(Q,K,V) = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_K}} \\right)V \\in \\mathbb{R}^{n \\times d_V} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f29f8f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDPA(nn.Module):\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        \n",
    "        d_K = K.size()[-1]\n",
    "        \n",
    "        scores = Q.matmul(K.transpose(-2, -1)) / np.sqrt(d_K)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask==0, -1e9)\n",
    "            \n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "        output = attention.matmul(V)\n",
    "        return output, attention\n",
    "    \n",
    "SDPA = SDPA()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fc95b8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDPA: Q[3, 30, 128]) K[3, 50, 128]) V[3, 50, 256]) \n",
      "=> output[3, 30, 256]) attention[3, 30, 50])\n"
     ]
    }
   ],
   "source": [
    "# SDPA\n",
    "\n",
    "n_batch = 3\n",
    "\n",
    "d_K = 128 # d_K(=d_Q) is not necessarily equal to d_V\n",
    "d_V = 256 \n",
    "\n",
    "n_Q = 30\n",
    "n_K = 50 # n_K must equal to n_V\n",
    "n_V = 50\n",
    "\n",
    "Q = torch.rand(n_batch, n_Q, d_K)\n",
    "K = torch.rand(n_batch, n_K, d_K)\n",
    "V = torch.rand(n_batch, n_V, d_V)\n",
    "\n",
    "# disregard n_batch,\n",
    "# Q.shape = (n_Q, d_K)\n",
    "# K.shape = (n_K, d_K)\n",
    "# matmul(Q, K.T).shape = attention.shape = (n_Q, n_K) = (30, 50)\n",
    "# V.shape = (n_V, d_V) = (50, 256)\n",
    "# matmul(attention, V).shape = output.shape = (n_Q, d_V) = (30, 256)\n",
    "\n",
    "output, attention = SDPA.forward(Q, K, V, mask=None)\n",
    "\n",
    "print(f'SDPA: Q{str(Q.shape)[11:]} K{str(K.shape)[11:]} V{str(V.shape)[11:]} \\n=> output{str(output.shape)[11:]} attention{str(attention.shape)[11:]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b2dda208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Head SDPA: Q[3, 5, 30, 128]) K[3, 5, 50, 128]) V[3, 5, 50, 256]) \n",
      "=> output[3, 5, 30, 256]) attention[3, 5, 30, 50])\n"
     ]
    }
   ],
   "source": [
    "# Multi-Head SDPA\n",
    "\n",
    "n_batch = 3\n",
    "\n",
    "n_head = 5\n",
    "\n",
    "d_K = 128\n",
    "d_V = 256\n",
    "\n",
    "n_Q = 30\n",
    "n_K = 50\n",
    "n_V = 50\n",
    "\n",
    "Q = torch.rand(n_batch, n_head, n_Q, d_K)\n",
    "K = torch.rand(n_batch, n_head, n_K, d_K)\n",
    "V = torch.rand(n_batch, n_head, n_V, d_V)\n",
    "\n",
    "output, attention = SDPA.forward(Q, K, V, mask=None)\n",
    "\n",
    "print(f'Multi-Head SDPA: Q{str(Q.shape)[11:]} K{str(K.shape)[11:]} V{str(V.shape)[11:]} \\n=> output{str(output.shape)[11:]} attention{str(attention.shape)[11:]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63662c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
