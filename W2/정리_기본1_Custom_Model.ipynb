{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b61b533",
   "metadata": {},
   "source": [
    "# PyTorch Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a11e94a",
   "metadata": {},
   "source": [
    "### torch.Tensor vs. torch.tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce701ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.Tensor \n",
    "# is a class.\n",
    "# if input data is torch.Tensor, shares the memory space.\n",
    "# if input data is list or numpy, copies the data.\n",
    "# transforms input data into 32-bit floating point data type.\n",
    "# also refers to the torch.Tensor data type??\n",
    "\n",
    "# torch.tensor \n",
    "# is a function.\n",
    "# always copies input data.\n",
    "# infers input data's data type, unless specified by dtype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38b6de43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.]) tensor([1.])\n",
      "tensor([2.]) tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# torch.Tensor - torch.Tensor input\n",
    "orig_data = torch.Tensor([1])\n",
    "\n",
    "new_data = torch.Tensor(orig_data)\n",
    "print(orig_data, new_data)\n",
    "\n",
    "orig_data[0] = 2\n",
    "print(orig_data, new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2391c0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] tensor([1.])\n",
      "[2] tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "# torch.Tensor - list, numpy input\n",
    "orig_data = [1]\n",
    "\n",
    "new_data = torch.Tensor(orig_data)\n",
    "print(orig_data, new_data)\n",
    "\n",
    "orig_data[0] = 2\n",
    "print(orig_data, new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfb1c01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.]) tensor([1.])\n",
      "tensor([2.]) tensor([1.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lh/13z6kjx97lsd31q5h0712_mm0000gn/T/ipykernel_20286/2575792912.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_data = torch.tensor(orig_data)\n"
     ]
    }
   ],
   "source": [
    "# torch.tensor - torch.Tensor input\n",
    "\n",
    "orig_data = torch.tensor([1])\n",
    "# orig_data = torch.Tensor([1]) # 처음 declare 할땐 뭐 쓰지? 노상관?\n",
    "new_data = torch.tensor(orig_data)\n",
    "print(orig_data, new_data)\n",
    "\n",
    "orig_data[0] = 2\n",
    "print(orig_data, new_data)\n",
    "\n",
    "# 워닝 내용은 대충 if you want to avoid a copy, use these functions 인듯."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bfea7264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.Tensor([1])\n",
    "type(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b535c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.Tensor([1])\n",
    "type(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc03a34",
   "metadata": {},
   "source": [
    "### index_select\n",
    "### axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e538a6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [3.]])\n",
      "tensor([1., 3.])\n"
     ]
    }
   ],
   "source": [
    "# index_select\n",
    "\n",
    "import torch\n",
    "\n",
    "A = torch.Tensor([[1, 2],\n",
    "                  [3, 4]])\n",
    "\n",
    "# TODO : [1, 3]을 만드세요!\n",
    "\n",
    "index = torch.tensor([0]) # must use int type for index\n",
    "output = torch.index_select(A, 1, index)\n",
    "print(output)\n",
    "\n",
    "output = output.reshape(-1)\n",
    "# output = output.view(-1) # tf에선 reshape인데 걍 reshape 쓰면 안됨?\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1cb64bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 3.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = A[:, 0] # 이걸로도 가능\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "898c1e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# axis test\n",
    "\n",
    "output = torch.index_select(A, 0, index)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ae6d4027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2],\n",
       "         [3, 4]],\n",
       "\n",
       "        [[5, 6],\n",
       "         [7, 8]]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# axis test - 3D\n",
    "\n",
    "A = torch.arange(1, 9)\n",
    "A = A.reshape(2, 2, 2)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5c456d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = torch.index_select(A, 0, index)\n",
    "output\n",
    "\n",
    "# check the axis orientation - from [1] to [5]\n",
    "# this is the 'most complicated' axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c8b3a9ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2]],\n",
       "\n",
       "        [[5, 6]]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = torch.index_select(A, 1, index)\n",
    "output\n",
    "\n",
    "# check the axis orientation - from [1] to [3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b1243112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1],\n",
       "         [3]],\n",
       "\n",
       "        [[5],\n",
       "         [7]]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = torch.index_select(A, 2, index)\n",
    "output\n",
    "\n",
    "# check the axis orientation - from [1] to [2]\n",
    "# this is the 'simplest' axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e5cc8344",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 2])\n",
      "tensor([[[ 1,  2],\n",
      "         [ 3,  4],\n",
      "         [ 5,  6]],\n",
      "\n",
      "        [[ 7,  8],\n",
      "         [ 9, 10],\n",
      "         [11, 12]],\n",
      "\n",
      "        [[13, 14],\n",
      "         [15, 16],\n",
      "         [17, 18]],\n",
      "\n",
      "        [[19, 20],\n",
      "         [21, 22],\n",
      "         [23, 24]]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.arange(1, 25)\n",
    "A = A.reshape(4, 3, 2)\n",
    "print(A.shape)\n",
    "print(A)\n",
    "\n",
    "# axis 0: 4\n",
    "# axis 1: 3\n",
    "# axis 2: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690a932a",
   "metadata": {},
   "source": [
    "### gather\n",
    "### view, reshape\n",
    "### expand, repeat\n",
    "### unsqueeze, squeeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "da412d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [4.]])\n",
      "tensor([1., 4.])\n"
     ]
    }
   ],
   "source": [
    "# 2D gather\n",
    "\n",
    "A = torch.Tensor([[1, 2],\n",
    "                  [3, 4]])\n",
    "\n",
    "index = torch.tensor([[0],\n",
    "                      [1]])\n",
    "output = torch.gather(A, 1, index)\n",
    "print(output)\n",
    "\n",
    "output = output.reshape(-1)\n",
    "print(output)\n",
    "\n",
    "# 'from [1] to [3]' axis 의 방향으로 첫번째 라인에서 index 0, \n",
    "# 1번째 라인에서 index 1에 해당하는 값들."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a1557c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.],\n",
      "         [4.]],\n",
      "\n",
      "        [[5.],\n",
      "         [8.]]])\n",
      "tensor([[1., 4.],\n",
      "        [5., 8.]])\n"
     ]
    }
   ],
   "source": [
    "# 3D gather\n",
    "\n",
    "A = torch.Tensor([[[1, 2],\n",
    "                   [3, 4]],\n",
    "                  \n",
    "                  [[5, 6],\n",
    "                   [7, 8]]])\n",
    "\n",
    "index = torch.tensor([[[0],\n",
    "                       [1]],\n",
    "                      [[0],\n",
    "                       [1]]])\n",
    "output = torch.gather(A, 2, index)\n",
    "print(output)\n",
    "# axis가 2임. The simplest axis. 'from [1] to [2]' axis.\n",
    "# 총 4개의 라인에 대해 index 0, 1, 0, 1에 해당하는 값을 gather.\n",
    "\n",
    "output = output.reshape(2, 2)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "7433de9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2, 3],\n",
      "         [4, 5, 6]]])\n",
      "gather_index:\n",
      "tensor([[[0],\n",
      "         [1]]])\n",
      "tensor([[1, 5]])\n"
     ]
    }
   ],
   "source": [
    "# 3D gather - arbitrary size\n",
    "\n",
    "# TODO : 임의의 크기의 3D tensor에서 대각선 요소 가져와 2D로 반환하는 함수를 만드세요! \n",
    "def get_diag_element_3D(A):\n",
    "    \n",
    "    C, H, W = A.shape\n",
    "    diag_size = min(H, W) \n",
    "    # C axis (channel axis = 0-axis = depth axis) is fixed.\n",
    "    # H-W plane에 diagonal line을 그리는 것.\n",
    "    \n",
    "    # gather_index = torch.arange(diag_size).view(diag_size, -1).expand(C, diag_size, 1)\n",
    "    gather_index = torch.arange(diag_size).reshape(diag_size, 1).expand(C, diag_size, 1)\n",
    "    # view vs. reshape: reshape doesn't impose any contiguity constraints,\n",
    "    # but also doesn't gunarantee data sharing. \n",
    "    print('gather_index:')\n",
    "    print(gather_index)\n",
    "    \n",
    "    output = torch.gather(A, 2, gather_index)\n",
    "    output = output.view(C, diag_size)\n",
    "\n",
    "    return output\n",
    "\n",
    "C = 1\n",
    "H = 2\n",
    "W = 3\n",
    "\n",
    "A = torch.tensor([i for i in range(1, C*H*W + 1)])\n",
    "A = A.reshape(C, H, W)\n",
    "print(A)\n",
    "\n",
    "A = get_diag_element_3D(A)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "1b6d6210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gather_index:\n",
      "tensor([[[0]]])\n",
      "🎉🎉🎉 성공!!! 🎉🎉🎉\n"
     ]
    }
   ],
   "source": [
    "# 아래 코드는 수정하실 필요가 없습니다!\n",
    "A = torch.tensor([[[1]]])\n",
    "\n",
    "if torch.all(get_diag_element_3D(A) == torch.Tensor([[1]])):\n",
    "    print(\"🎉🎉🎉 성공!!! 🎉🎉🎉\")\n",
    "else:\n",
    "    print(\"🦆 다시 도전해봐요!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "2d4e2751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gather_index:\n",
      "tensor([[[0],\n",
      "         [1]],\n",
      "\n",
      "        [[0],\n",
      "         [1]]])\n",
      "🎉🎉🎉 성공!!! 🎉🎉🎉\n"
     ]
    }
   ],
   "source": [
    "# 아래 코드는 수정하실 필요가 없습니다!\n",
    "A = torch.Tensor([[[1, 2],\n",
    "                   [3, 4]],\n",
    "                  [[5, 6],\n",
    "                   [7, 8]]])\n",
    "\n",
    "if torch.all(get_diag_element_3D(A) == torch.Tensor([[1, 4],\n",
    "                                                     [5, 8]])):\n",
    "    print(\"🎉🎉🎉 성공!!! 🎉🎉🎉\")\n",
    "else:\n",
    "    print(\"🦆 다시 도전해봐요!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a11f91fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gather_index:\n",
      "tensor([[[0],\n",
      "         [1]]])\n",
      "🎉🎉🎉 성공!!! 🎉🎉🎉\n"
     ]
    }
   ],
   "source": [
    "# 아래 코드는 수정하실 필요가 없습니다!\n",
    "A = torch.Tensor([[[1, 2, 3],\n",
    "                   [4, 5, 6]]])\n",
    "\n",
    "if torch.all(get_diag_element_3D(A) == torch.Tensor([[1, 5]])):\n",
    "    print(\"🎉🎉🎉 성공!!! 🎉🎉🎉\")\n",
    "else:\n",
    "    print(\"🦆 다시 도전해봐요!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "62d981c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gather_index:\n",
      "tensor([[[0],\n",
      "         [1],\n",
      "         [2]],\n",
      "\n",
      "        [[0],\n",
      "         [1],\n",
      "         [2]]])\n",
      "🎉🎉🎉 성공!!! 🎉🎉🎉\n"
     ]
    }
   ],
   "source": [
    "# 아래 코드는 수정하실 필요가 없습니다!\n",
    "A = torch.tensor([[[ 1,  2,  3,  4,  5],\n",
    "                   [ 6,  7,  8,  9, 10],\n",
    "                   [11, 12, 13, 14, 15]],\n",
    "          \n",
    "                  [[16, 17, 18, 19, 20],\n",
    "                   [21, 22, 23, 24, 25],\n",
    "                   [26, 27, 28, 29, 30]]])\n",
    "\n",
    "if torch.all(get_diag_element_3D(A) == torch.Tensor([[ 1,  7, 13],\n",
    "                                                     [16, 22, 28]])):\n",
    "    print(\"🎉🎉🎉 성공!!! 🎉🎉🎉\")\n",
    "else:\n",
    "    print(\"🦆 다시 도전해봐요!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "136771bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gather_index:\n",
      "tensor([[[0],\n",
      "         [1],\n",
      "         [2]],\n",
      "\n",
      "        [[0],\n",
      "         [1],\n",
      "         [2]],\n",
      "\n",
      "        [[0],\n",
      "         [1],\n",
      "         [2]]])\n",
      "🎉🎉🎉 성공!!! 🎉🎉🎉\n"
     ]
    }
   ],
   "source": [
    "# 아래 코드는 수정하실 필요가 없습니다!\n",
    "A = torch.tensor([[[ 1,  2,  3],\n",
    "                   [ 4,  5,  6],\n",
    "                   [ 7,  8,  9],\n",
    "                   [10, 11, 12],\n",
    "                   [13, 14, 15]],\n",
    "        \n",
    "                  [[16, 17, 18],\n",
    "                   [19, 20, 21],\n",
    "                   [22, 23, 24],\n",
    "                   [25, 26, 27],\n",
    "                   [28, 29, 30]],\n",
    "        \n",
    "                  [[31, 32, 33],\n",
    "                   [34, 35, 36],\n",
    "                   [37, 38, 39],\n",
    "                   [40, 41, 42],\n",
    "                   [43, 44, 45]]])\n",
    "\n",
    "if torch.all(get_diag_element_3D(A) == torch.Tensor([[ 1,  5,  9],\n",
    "                                                     [16, 20, 24],\n",
    "                                                     [31, 35, 39]])):\n",
    "    print(\"🎉🎉🎉 성공!!! 🎉🎉🎉\")\n",
    "else:\n",
    "    print(\"🦆 다시 도전해봐요!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e6b8c01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 2])\n",
      "torch.Size([2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "A = torch.arange(1, 9)\n",
    "A = A.reshape(2, 2, 2)\n",
    "print(A.size())\n",
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a4f68200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1])\n",
      "tensor([[0],\n",
      "        [1]])\n",
      "tensor([[[0],\n",
      "         [1]]])\n"
     ]
    }
   ],
   "source": [
    "# gather_index analysis\n",
    "\n",
    "C = 1\n",
    "H = 2\n",
    "W = 3\n",
    "\n",
    "diag_size = min(H, W) \n",
    "    \n",
    "# A = torch.tensor([i for i in range(1, C*H*W + 1)])\n",
    "# A = A.reshape(C, H, W)\n",
    "\n",
    "gather_index = torch.arange(diag_size)\n",
    "print(gather_index)\n",
    "\n",
    "gather_index = torch.arange(diag_size).reshape(diag_size, 1)\n",
    "print(gather_index)\n",
    "\n",
    "gather_index = torch.arange(diag_size).reshape(diag_size, 1).expand(C, diag_size, 1)\n",
    "print(gather_index)\n",
    "\n",
    "# output = torch.gather(A, 2, gather_index)    \n",
    "# output = output.view(C, diag_size)\n",
    "\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "e4c73252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1],\n",
      "        [2]])\n",
      "tensor([[1, 1, 1],\n",
      "        [2, 2, 2]])\n",
      "tensor([[1, 1, 1, 1],\n",
      "        [2, 2, 2, 2]])\n",
      "tensor([[1, 1, 1],\n",
      "        [2, 2, 2],\n",
      "        [1, 1, 1],\n",
      "        [2, 2, 2]])\n"
     ]
    }
   ],
   "source": [
    "# expand vs. repeat\n",
    "\n",
    "t = torch.arange(1, 3)\n",
    "t = t.reshape(2, 1)\n",
    "print(t)\n",
    "\n",
    "# expand\n",
    "print(t.expand(2, 3)) # expand as a specified shape.\n",
    "print(t.expand(-1, 4)) # -1: do not change this axis.\n",
    "\n",
    "# repeat\n",
    "print(t.repeat(2, 3)) # repeat as many times as."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "8d79a365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1]],\n",
      "\n",
      "        [[2]],\n",
      "\n",
      "        [[3]]])\n",
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# squeeze vs. unsqueeze\n",
    "\n",
    "t = torch.arange(1, 4)\n",
    "t = t.reshape(3, 1, 1)\n",
    "print(t)\n",
    "t = t.squeeze() # 크기가 1인 차원을 제거\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "53f63473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "tensor([1, 2, 3])\n",
      "torch.Size([1, 3])\n",
      "tensor([[1, 2, 3]])\n",
      "torch.Size([1, 1, 3])\n",
      "tensor([[[1, 2, 3]]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.arange(1, 4)\n",
    "print(t.shape)\n",
    "print(t)\n",
    "\n",
    "t = t.unsqueeze(0) # 0-axis is always the 'most complicated' axis\n",
    "print(t.shape)\n",
    "print(t)\n",
    "\n",
    "t = t.unsqueeze(0)\n",
    "print(t.shape)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44be424c",
   "metadata": {},
   "source": [
    "# nn.Module 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "3b5ae253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get two numbers and add them together\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Add(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        output = torch.add(x1, x2)\n",
    "        \n",
    "        return output\n",
    "\n",
    "x1 = torch.tensor([1])\n",
    "x2 = torch.tensor([2])\n",
    "\n",
    "add = Add()\n",
    "\n",
    "output = add(x1, x2)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e71f0d7",
   "metadata": {},
   "source": [
    "### Module, nn.ModuleList, nn.ModuleDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "b8196c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sequential\n",
    "# variable pre-run assignment in __init__()\n",
    "\n",
    "class Add(nn.Module):\n",
    "    def __init__(self, value): # value is initiated before run\n",
    "        super().__init__()\n",
    "        self.value = value # self.value\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.value\n",
    "\n",
    "calculator = nn.Sequential(Add(3), # collection of modules\n",
    "                           Add(2),\n",
    "                           Add(5))\n",
    "\n",
    "x = torch.tensor([1])\n",
    "\n",
    "output = calculator(x)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "82f7857a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ModuleList\n",
    "\n",
    "class Add(nn.Module):\n",
    "    def __init__(self, value):\n",
    "        super().__init__()\n",
    "        self.value = value\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.value\n",
    "\n",
    "class Calculator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # nn.ModuleList()\n",
    "        self.add_list = nn.ModuleList([Add(2), Add(3), Add(5)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.add_list[1](x)\n",
    "        x = self.add_list[0](x)\n",
    "        x = self.add_list[2](x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "x = torch.tensor([1])\n",
    "\n",
    "calculator = Calculator()\n",
    "output = calculator(x)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "d4cc1f02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ModuleDict\n",
    "\n",
    "class Add(nn.Module):\n",
    "    def __init__(self, value):\n",
    "        super().__init__()\n",
    "        self.value = value\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.value\n",
    "\n",
    "class Calculator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.add_dict = nn.ModuleDict({'add2': Add(2),\n",
    "                                       'add3': Add(3),\n",
    "                                       'add5': Add(5)})\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.add_dict['add3'](x)\n",
    "        x = self.add_dict['add2'](x)\n",
    "        x = self.add_dict['add5'](x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "x = torch.tensor([1])\n",
    "\n",
    "calculator = Calculator()\n",
    "output = calculator(x)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "27a862bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Function A Initialized\n",
      "        Function B Initialized\n",
      "    Layer AB Initialized\n",
      "        Function C Initialized\n",
      "        Function D Initialized\n",
      "    Layer CD Initialized\n",
      "Model ABCD Initialized\n",
      "\n",
      "Model ABCD started\n",
      "    Layer AB started\n",
      "        Function A started\n",
      "        Function A done\n",
      "        Function B started\n",
      "        Function B done\n",
      "    Layer AB done\n",
      "    Layer CD started\n",
      "        Function C started\n",
      "        Function C done\n",
      "        Function D started\n",
      "        Function D done\n",
      "    Layer CD done\n",
      "Model ABCD done\n",
      "\n",
      "🎉🎉🎉 모든 딥러닝 모델은 이처럼 Module들이 쌓이고 쌓여서 만들어집니다! 🎉🎉🎉\n",
      "🎉🎉🎉 흐름을 느껴보시고 이 흐름이 이해가 되신 분은 다음으로 가시면 됩니다! 🎉🎉\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "# Function\n",
    "class Function_A(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        print(f\"        Function A Initialized\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"        Function A started\")\n",
    "        print(f\"        Function A done\")\n",
    "\n",
    "class Function_B(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        print(f\"        Function B Initialized\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"        Function B started\")\n",
    "        print(f\"        Function B done\")\n",
    "\n",
    "class Function_C(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        print(f\"        Function C Initialized\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"        Function C started\")\n",
    "        print(f\"        Function C done\")\n",
    "\n",
    "class Function_D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        print(f\"        Function D Initialized\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"        Function D started\")\n",
    "        print(f\"        Function D done\")\n",
    "\n",
    "\n",
    "# Layer\n",
    "class Layer_AB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.a = Function_A()\n",
    "        self.b = Function_B()\n",
    "\n",
    "        print(f\"    Layer AB Initialized\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"    Layer AB started\")\n",
    "        self.a(x)\n",
    "        self.b(x)\n",
    "        print(f\"    Layer AB done\")\n",
    "\n",
    "class Layer_CD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.c = Function_C()\n",
    "        self.d = Function_D()\n",
    "\n",
    "        print(f\"    Layer CD Initialized\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"    Layer CD started\")\n",
    "        self.c(x)\n",
    "        self.d(x)\n",
    "        print(f\"    Layer CD done\")\n",
    "\n",
    "\n",
    "# Model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ab = Layer_AB()\n",
    "        self.cd = Layer_CD()\n",
    "\n",
    "        print(f\"Model ABCD Initialized\\n\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"Model ABCD started\")\n",
    "        self.ab(x)\n",
    "        self.cd(x)\n",
    "        print(f\"Model ABCD done\\n\")\n",
    "\n",
    "\n",
    "x = torch.tensor([7])\n",
    "\n",
    "model = Model()\n",
    "model(x)\n",
    "\n",
    "print(\"🎉🎉🎉 모든 딥러닝 모델은 이처럼 Module들이 쌓이고 쌓여서 만들어집니다! 🎉🎉🎉\")\n",
    "print(\"🎉🎉🎉 흐름을 느껴보시고 이 흐름이 이해가 되신 분은 다음으로 가시면 됩니다! 🎉🎉\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36a99f4",
   "metadata": {},
   "source": [
    "### Parameter, Buffer, state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "167dfbb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 4., 4.],\n",
       "        [8., 8., 8.]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameter. 미리 만들어진 tensor들을 nn.Module 안에 보관.\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        \n",
    "        # initialize as parameter\n",
    "        self.W = Parameter(torch.ones((out_features, in_features)))\n",
    "        self.b = Parameter(torch.ones(out_features))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = torch.addmm(self.b, x, self.W.T)\n",
    "    \n",
    "        return output\n",
    "    \n",
    "x = torch.Tensor([[1, 2],\n",
    "                  [3, 4]])\n",
    "\n",
    "linear = Linear(2, 3)\n",
    "output = linear(x)\n",
    "output\n",
    "\n",
    "# Parameter를 사용해야만 output tensor에 gradient를 계산하는 grad_fn이 생성됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "0fad6115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('W',\n",
       "              tensor([[1., 1.],\n",
       "                      [1., 1.],\n",
       "                      [1., 1.]])),\n",
       "             ('b', tensor([1., 1., 1.]))])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# state_dict()\n",
    "\n",
    "linear.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "686919fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7.])\n",
      "OrderedDict([('parameter', tensor([7.])), ('buffer', tensor([7.]))])\n"
     ]
    }
   ],
   "source": [
    "# Buffer. Parameter는 아니지만 저장할 수 있는 tensor.\n",
    "# 모델을 저장할 때 같이 저장됨.\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.parameter = Parameter(torch.Tensor([7]))\n",
    "        self.tensor = torch.Tensor([7])\n",
    "        self.register_buffer('buffer', torch.Tensor([7]), persistent=True)\n",
    "        \n",
    "model = Model()\n",
    "\n",
    "buffer = model.get_buffer('buffer')\n",
    "print(buffer)\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcd4f6c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.5720], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "\n",
    "# 아래 코드는 수정하실 필요가 없습니다!\n",
    "# 하지만 아래 과제를 진행하기 전에 아래 코드를 보면서 최대한 이해해보세요!\n",
    "\n",
    "# Function\n",
    "class Function_A(nn.Module):\n",
    "    def __init__(self, name):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * 2\n",
    "        return x\n",
    "\n",
    "class Function_B(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.W1 = Parameter(torch.Tensor([10]))\n",
    "        self.W2 = Parameter(torch.Tensor([2]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x / self.W1\n",
    "        x = x / self.W2\n",
    "\n",
    "        return x\n",
    "\n",
    "class Function_C(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.register_buffer('duck', torch.Tensor([7]), persistent=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * self.duck\n",
    "        \n",
    "        return x\n",
    "\n",
    "class Function_D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.W1 = Parameter(torch.Tensor([3]))\n",
    "        self.W2 = Parameter(torch.Tensor([5]))\n",
    "        self.c = Function_C()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.W1\n",
    "        x = self.c(x)\n",
    "        x = x / self.W2\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Layer\n",
    "class Layer_AB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.a = Function_A('duck')\n",
    "        self.b = Function_B()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.a(x) / 5\n",
    "        x = self.b(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Layer_CD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.c = Function_C()\n",
    "        self.d = Function_D()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c(x)\n",
    "        x = self.d(x) + 1\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ab = Layer_AB()\n",
    "        self.cd = Layer_CD()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ab(x)\n",
    "        x = self.cd(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "x = torch.tensor([7])\n",
    "\n",
    "model = Model()\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50ee6e5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.5720])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = (((x*2/5/10/2*7)+3)*7/5)+1\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcdc3ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('ab.b.W1', tensor([10.])),\n",
       "             ('ab.b.W2', tensor([2.])),\n",
       "             ('cd.c.duck', tensor([7.])),\n",
       "             ('cd.d.W1', tensor([3.])),\n",
       "             ('cd.d.W2', tensor([5.])),\n",
       "             ('cd.d.c.duck', tensor([7.]))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086e4671",
   "metadata": {},
   "source": [
    "### named_modules, named_children\n",
    "### named_parameters, parameters\n",
    "### named_buffers, buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cba13ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Name ] : \n",
      "[ Module ]\n",
      "Model(\n",
      "  (ab): Layer_AB(\n",
      "    (a): Function_A()\n",
      "    (b): Function_B()\n",
      "  )\n",
      "  (cd): Layer_CD(\n",
      "    (c): Function_C()\n",
      "    (d): Function_D(\n",
      "      (c): Function_C()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "------------------------------\n",
      "[ Name ] : ab\n",
      "[ Module ]\n",
      "Layer_AB(\n",
      "  (a): Function_A()\n",
      "  (b): Function_B()\n",
      ")\n",
      "------------------------------\n",
      "[ Name ] : ab.a\n",
      "[ Module ]\n",
      "Function_A()\n",
      "------------------------------\n",
      "[ Name ] : ab.b\n",
      "[ Module ]\n",
      "Function_B()\n",
      "------------------------------\n",
      "[ Name ] : cd\n",
      "[ Module ]\n",
      "Layer_CD(\n",
      "  (c): Function_C()\n",
      "  (d): Function_D(\n",
      "    (c): Function_C()\n",
      "  )\n",
      ")\n",
      "------------------------------\n",
      "[ Name ] : cd.c\n",
      "[ Module ]\n",
      "Function_C()\n",
      "------------------------------\n",
      "[ Name ] : cd.d\n",
      "[ Module ]\n",
      "Function_D(\n",
      "  (c): Function_C()\n",
      ")\n",
      "------------------------------\n",
      "[ Name ] : cd.d.c\n",
      "[ Module ]\n",
      "Function_C()\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# named_modules()\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    print(f\"[ Name ] : {name}\\n[ Module ]\\n{module}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6363d8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Name ] : ab\n",
      "[ Children ]\n",
      "Layer_AB(\n",
      "  (a): Function_A()\n",
      "  (b): Function_B()\n",
      ")\n",
      "------------------------------\n",
      "[ Name ] : cd\n",
      "[ Children ]\n",
      "Layer_CD(\n",
      "  (c): Function_C()\n",
      "  (d): Function_D(\n",
      "    (c): Function_C()\n",
      "  )\n",
      ")\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# named_children()\n",
    "\n",
    "for name, child in model.named_children():\n",
    "    print(f\"[ Name ] : {name}\\n[ Children ]\\n{child}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b34474ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Function_A()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_submodule()\n",
    "\n",
    "submodule = model.get_submodule('ab.a')\n",
    "submodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db25d0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Name ] : ab.b.W1\n",
      "[ Parameter ]\n",
      "Parameter containing:\n",
      "tensor([10.], requires_grad=True)\n",
      "------------------------------\n",
      "[ Name ] : ab.b.W2\n",
      "[ Parameter ]\n",
      "Parameter containing:\n",
      "tensor([2.], requires_grad=True)\n",
      "------------------------------\n",
      "[ Name ] : cd.d.W1\n",
      "[ Parameter ]\n",
      "Parameter containing:\n",
      "tensor([3.], requires_grad=True)\n",
      "------------------------------\n",
      "[ Name ] : cd.d.W2\n",
      "[ Parameter ]\n",
      "Parameter containing:\n",
      "tensor([5.], requires_grad=True)\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# named_parameters()\n",
    "\n",
    "for name, parameter in model.named_parameters():\n",
    "    print(f\"[ Name ] : {name}\\n[ Parameter ]\\n{parameter}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be86012a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([10.], requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_parameter()\n",
    "\n",
    "parameter = model.get_parameter('ab.b.W1')\n",
    "parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a99a6409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Name ] : cd.c.duck\n",
      "[ Buffer ] : tensor([7.])\n",
      "------------------------------\n",
      "[ Name ] : cd.d.c.duck\n",
      "[ Buffer ] : tensor([7.])\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# named_buffers()\n",
    "\n",
    "for name, buffer in model.named_buffers():\n",
    "    print(f\"[ Name ] : {name}\\n[ Buffer ] : {buffer}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "854cb141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Buffer ] : tensor([7.])\n",
      "------------------------------\n",
      "[ Buffer ] : tensor([7.])\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# buffers()\n",
    "\n",
    "for buffer in model.buffers():\n",
    "    print(f\"[ Buffer ] : {buffer}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d6150c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buffer = model.get_buffer('cd.c.duck')\n",
    "buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee003b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (ab): Layer_AB(\n",
       "    (a): Function_A(name=duck)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extra_repr()\n",
    "\n",
    "class Function_A(nn.Module):\n",
    "    def __init__(self, name):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * 2\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f'name={self.name}'\n",
    "    \n",
    "class Layer_AB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.a = Function_A('duck')\n",
    "        # self.b = Function_B()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.a(x) / 5\n",
    "        x = self.b(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ab = Layer_AB()\n",
    "        # self.cd = Layer_CD()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ab(x)\n",
    "        # x = self.cd(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = Model()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5dea7e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "['running_mean', 'running_var', 'num_batches_tracked']\n"
     ]
    }
   ],
   "source": [
    "# parameters(), buffers(), named_buffers()\n",
    "\n",
    "module = nn.BatchNorm1d(10)\n",
    "\n",
    "parameter_n = len(list(module.parameters()))\n",
    "buffer_n = len(list(module.buffers()))\n",
    "\n",
    "print(parameter_n)\n",
    "print(buffer_n)\n",
    "\n",
    "buffer_names = [name for name, _ in module.named_buffers()]\n",
    "print(buffer_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "752402ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Name ] : weight\n",
      "[ Parameter ]\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "------------------------------\n",
      "[ Name ] : bias\n",
      "[ Parameter ]\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# named_parameters()\n",
    "\n",
    "for name, parameter in module.named_parameters():\n",
    "    print(f\"[ Name ] : {name}\\n[ Parameter ]\\n{parameter}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5569348b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Name ] : running_mean\n",
      "[ Buffer ] : tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "------------------------------\n",
      "[ Name ] : running_var\n",
      "[ Buffer ] : tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "------------------------------\n",
      "[ Name ] : num_batches_tracked\n",
      "[ Buffer ] : 0\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# named_buffers()\n",
    "\n",
    "for name, buffer in module.named_buffers():\n",
    "    print(f\"[ Name ] : {name}\\n[ Buffer ] : {buffer}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "43113f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight', tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('bias', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('running_mean',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('running_var', tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('num_batches_tracked', tensor(0))])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# state_dict()\n",
    "\n",
    "module.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcbac4b",
   "metadata": {},
   "source": [
    "### hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f7cb43e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program A processing!\n",
      "program B processing!\n"
     ]
    }
   ],
   "source": [
    "# 패키지화 된 코드에서 다른 프로그래머가\n",
    "# custom 코드를 중간에 실행시킬 수 있도록 만들어놓은 인터페이스\n",
    "\n",
    "# 프로그램의 실행 로직을 분석하거나\n",
    "# 프로그램에 추가적인 기능을 제공할 때 사용\n",
    "\n",
    "def program_A(x):\n",
    "    print('program A processing!')\n",
    "    return x + 3\n",
    "\n",
    "def program_B(x):\n",
    "    print('program B processing!')\n",
    "    return x - 3\n",
    "\n",
    "class Package(object):\n",
    "    def __init__(self):\n",
    "        self.programs = [program_A, program_B]\n",
    "        \n",
    "        # 이렇게 Package에는 self.hook이란 변수를 만들어줘야 함\n",
    "        # 현재는 hook에 아무 것도 없는 상태\n",
    "        self.hooks = []\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for program in self.programs:\n",
    "            x = program(x)\n",
    "\n",
    "            # Package를 사용하는 사람이 자신만의 custom program을\n",
    "            # 등록할 수 있도록 미리 만들어놓은 인터페이스 hook\n",
    "            if self.hooks:\n",
    "                for hook in self.hooks:\n",
    "                    output = hook(x)\n",
    "\n",
    "                    if output:\n",
    "                        x = output\n",
    "\n",
    "        return x\n",
    "\n",
    "package = Package()\n",
    "\n",
    "input = 3\n",
    "output = package(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "86223872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program A processing!\n",
      "hook for analysis, current value is 6\n",
      "program B processing!\n",
      "hook for analysis, current value is 3\n",
      "output: 3\n"
     ]
    }
   ],
   "source": [
    "# hook으로 로직 분석하기\n",
    "\n",
    "# module 내부에 print문 찍어주는 효과\n",
    "\n",
    "def hook_analysis(x):\n",
    "    print(f'hook for analysis, current value is {x}')\n",
    "\n",
    "package.hooks = [] # 이게 왜 필요함???\n",
    "package.hooks.append(hook_analysis)\n",
    "\n",
    "input = 3\n",
    "output = package(input)\n",
    "print(f'output: {output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "620757aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program A processing!\n",
      "hook for multiplying\n",
      "program B processing!\n",
      "hook for multiplying\n",
      "output: 45\n"
     ]
    }
   ],
   "source": [
    "# hook으로 기능 추가하기\n",
    "\n",
    "def hook_multiply(x):\n",
    "    print('hook for multiplying')\n",
    "    return x * 3\n",
    "\n",
    "package.hooks = []\n",
    "package.hooks.append(hook_multiply)\n",
    "\n",
    "input = 3\n",
    "output = package(input)\n",
    "print(f'output: {output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5c1451e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program A processing!\n",
      "hook for multiplying\n",
      "hook for analysis, current value is 18\n",
      "program B processing!\n",
      "hook for multiplying\n",
      "hook for analysis, current value is 45\n",
      "output: 45\n"
     ]
    }
   ],
   "source": [
    "# hook에 여러 개의 기능 넣기\n",
    "\n",
    "package.hooks = []\n",
    "package.hooks.append(hook_multiply)\n",
    "package.hooks.append(hook_analysis)\n",
    "\n",
    "input = 3\n",
    "output = package(input)\n",
    "\n",
    "print(f'output: {output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "660db12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_hook()\n",
    "\n",
    "# 프로그램 실행 전에 넣으나 후에 넣으나 설계자 마음\n",
    "\n",
    "def program_A(x):\n",
    "    print('program A processing!')\n",
    "    return x + 3\n",
    "\n",
    "def program_B(x):\n",
    "    print('program B processing!')\n",
    "    return x - 3\n",
    "\n",
    "class Package(object):\n",
    "    def __init__(self):\n",
    "        self.programs = [program_A, program_B]\n",
    "\n",
    "        # pre_hooks\n",
    "        self.pre_hooks = []\n",
    "        \n",
    "        # hooks\n",
    "        self.hooks = []\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for program in self.programs:\n",
    "            \n",
    "            # pre_hooks\n",
    "            if self.pre_hooks:\n",
    "                for hook in self.pre_hooks:\n",
    "                    output = hook(x)\n",
    "                    if output:\n",
    "                        x = output\n",
    "\n",
    "            x = program(x)\n",
    "\n",
    "            # hooks\n",
    "            if self.hooks:\n",
    "                for hook in self.hooks:\n",
    "                    output = hook(x)\n",
    "                    if output:\n",
    "                        x = output\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83130dac",
   "metadata": {},
   "source": [
    "### hook in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65faa603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hook for Tensor\n",
    "    # backward hook\n",
    "        # function to use: tensor.register_hook(hook)\n",
    "\n",
    "# hook for Module\n",
    "    # forward pre hook\n",
    "        # function to use: register_forward_pre_hook(hook)\n",
    "    # forward hook\n",
    "        # function to use: register_forward_hook(hook)\n",
    "    # backward hook\n",
    "        # function to use: register_backward_hook(hook)\n",
    "    # full backward hook\n",
    "        # function to use: register_full_backward_hook(hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5312ebfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(1, <function __main__.tensor_hook(grad)>)])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# register_hook\n",
    "# _backward_hooks # ???\n",
    "\n",
    "import torch\n",
    "\n",
    "t = torch.rand(1, requires_grad=True)\n",
    "\n",
    "def tensor_hook(grad):\n",
    "    pass\n",
    "\n",
    "t.register_hook(tensor_hook)\n",
    "\n",
    "t._backward_hooks # ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4de862f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': True,\n",
       " '_parameters': OrderedDict(),\n",
       " '_buffers': OrderedDict(),\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_hooks': OrderedDict([(4, <function __main__.module_hook(grad)>)]),\n",
       " '_is_full_backward_hook': True,\n",
       " '_forward_hooks': OrderedDict([(3, <function __main__.module_hook(grad)>)]),\n",
       " '_forward_pre_hooks': OrderedDict([(2,\n",
       "               <function __main__.module_hook(grad)>)]),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_modules': OrderedDict()}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# register_forward_pre_hook()\n",
    "# register_forward_hook()\n",
    "# register_full_backward_hook()\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "def module_hook(grad):\n",
    "    pass\n",
    "    \n",
    "model = Model()\n",
    "model.register_forward_pre_hook(module_hook)\n",
    "model.register_forward_hook(module_hook)\n",
    "model.register_full_backward_hook(module_hook)\n",
    "\n",
    "model.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cd0b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward_pre_hooks\n",
    "# forward_hooks\n",
    "# backward_hooks # deprecated # so backward_hooks is used for tensor only?\n",
    "# full_backward_hooks\n",
    "# state_dict_hooks # used internally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a8242063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.8208]), tensor([0.0552]), tensor(0.8760)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward_pre_hook\n",
    "# forward_hook\n",
    "# 으로 값 저장하기\n",
    "\n",
    "class Add(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        output = torch.add(x1, x2)\n",
    "\n",
    "        return output\n",
    "\n",
    "add = Add()\n",
    "\n",
    "answer = []\n",
    "\n",
    "def pre_hook(module, input):\n",
    "    answer.extend(input)\n",
    "\n",
    "def hook(module, input, output):\n",
    "    answer.extend(output)\n",
    "\n",
    "add.register_forward_pre_hook(pre_hook)\n",
    "add.register_forward_hook(hook)\n",
    "\n",
    "x1 = torch.rand(1)\n",
    "x2 = torch.rand(1)\n",
    "\n",
    "output = add(x1, x2)\n",
    "\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2e7e499c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9335])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([5.9335])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hook으로 값 수정하기\n",
    "\n",
    "class Add(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        output = torch.add(x1, x2)\n",
    "\n",
    "        return output\n",
    "\n",
    "add = Add()\n",
    "\n",
    "def hook(module, input, output):\n",
    "    return output + 5\n",
    "\n",
    "add.register_forward_hook(hook)\n",
    "\n",
    "x1 = torch.rand(1)\n",
    "x2 = torch.rand(1)\n",
    "\n",
    "output = add(x1, x2)\n",
    "print(x1+x2)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "270981bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([4.8898]), tensor([2.8644]), tensor([1.])]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list에 backprop gradient값 저장하기\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.W = Parameter(torch.Tensor([5]))\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        output = x1 * x2\n",
    "        output = output * self.W\n",
    "\n",
    "        return output\n",
    "\n",
    "model = Model()\n",
    "\n",
    "answer = []\n",
    "\n",
    "# TODO : hook를 이용해서 x1.grad, x2.grad, output.grad 값을 알아내 answer에 저장하세요\n",
    "def module_hook(module, grad_input, grad_output):\n",
    "    answer.extend(grad_input)\n",
    "    answer.extend(grad_output)\n",
    "\n",
    "# full backward hook\n",
    "model.register_full_backward_hook(module_hook)\n",
    "\n",
    "\n",
    "x1 = torch.rand(1, requires_grad=True)\n",
    "x2 = torch.rand(1, requires_grad=True)\n",
    "\n",
    "output = model(x1, x2)\n",
    "output.retain_grad() \n",
    "# Tensor.retain_grad() -> None\n",
    "# Enables this tensor to have their grad populating during backward()\n",
    "output.backward() # backprop\n",
    "\n",
    "answer # [x1.grad, x2.grad, output.grad]\n",
    "\n",
    "# ???\n",
    "# x랑 w를 이용해 y_hat 값을 구하고 그걸 y가 포함된 cost function에 넣어서\n",
    "# cost function의 w에 대한 gradient를 구하는게 layer 아닌가?\n",
    "# y랑 cost function이 없는데 어떻게 gradient를 구하는거지?\n",
    "\n",
    "# -> 여기에 넣는 계산식은 forward prop이 아니라 cost function임.\n",
    "# d(output) / d(input) 을 구하는게 목적.\n",
    "\n",
    "# -> 기본적으로 input을 output으로 변화시키는 식이 존재하니까,\n",
    "# input에 대한 output의 미분값을 구하는 것.\n",
    "# 이건 label이 없어도 가능.\n",
    "# 즉, forward prop과 cost function은 본질적으로 다르지 않다. \n",
    "# 끝에 cost function만 붙이면 됨.\n",
    "# 첫번째 질문에 대한 답: cost function도 그냥 function이고, \n",
    "# 결국 input과 식을 줬을 때 output을 계산하고, \n",
    "# d(output) / d(input) 을 계산하는 구조라는 것.\n",
    "# label과 cost function을 끝에 추가해 완성시켜 주면,\n",
    "# d(output, i.e. cost) / d(input, i.e. weight) 를 알아내고,\n",
    "# 거기다 learning rate를 곱해 weight를 update할 amount를 정하는 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "50033fbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.3045)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# module 단위는 backward hook은 input gradient, output gradient값만 가져와서\n",
    "# module 내부의 tensor의 gradient값은 알아낼 수 없음.\n",
    "# module의 Parameter W의 gradient값을 알아내기 위해\n",
    "# tensor 단위의 backward hook 사용해야.\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.W = Parameter(torch.Tensor([5]))\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        output = x1 * x2\n",
    "        output = output * self.W\n",
    "\n",
    "        return output\n",
    "\n",
    "model = Model()\n",
    "\n",
    "answer = []\n",
    "\n",
    "# TODO : hook를 이용해서 W의 gradient 값을 알아내 answer에 저장하세요\n",
    "def tensor_hook(grad):\n",
    "    answer.extend(grad)\n",
    "\n",
    "# model.W에 register_hook() 적용\n",
    "model.W.register_hook(tensor_hook)\n",
    "\n",
    "\n",
    "x1 = torch.rand(1, requires_grad=True)\n",
    "x2 = torch.rand(1, requires_grad=True)\n",
    "\n",
    "output = model(x1, x2)\n",
    "output.backward()\n",
    "\n",
    "answer # [model.W.grad]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baeb4b2",
   "metadata": {},
   "source": [
    "### apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d9614148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=2, out_features=2, bias=True)\n",
      "Parameter containing:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "Linear(in_features=2, out_features=2, bias=True)\n",
      "Parameter containing:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (1): Linear(in_features=2, out_features=2, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
       "  (1): Linear(in_features=2, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "@torch.no_grad() # ???\n",
    "def init_weights(m):\n",
    "    print(m) # ???\n",
    "    if type(m) == nn.Linear:\n",
    "        m.weight.fill_(1.0)\n",
    "        print(m.weight) # ???\n",
    "\n",
    "net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
    "net.apply(init_weights) # ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "82ee289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "\n",
    "# 아래 코드는 수정하실 필요가 없습니다!\n",
    "# 하지만 아래 과제를 진행하기 전에 아래 코드를 보면서 최대한 이해해보세요!\n",
    "\n",
    "# Function\n",
    "class Function_A(nn.Module):\n",
    "    def __init__(self, name):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.W = Parameter(torch.rand(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.W\n",
    "\n",
    "class Function_B(nn.Module):\n",
    "    def __init__(self, name):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.W = Parameter(torch.rand(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x - self.W\n",
    "\n",
    "class Function_C(nn.Module):\n",
    "    def __init__(self, name):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.W = Parameter(torch.rand(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.W\n",
    "\n",
    "class Function_D(nn.Module):\n",
    "    def __init__(self, name):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.W = Parameter(torch.rand(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x / self.W\n",
    "\n",
    "\n",
    "# Layer\n",
    "class Layer_AB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.a = Function_A('plus')\n",
    "        self.b = Function_B('substract')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.a(x)\n",
    "        x = self.b(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Layer_CD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.c = Function_C('multiply')\n",
    "        self.d = Function_D('divide')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c(x)\n",
    "        x = self.d(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ab = Layer_AB()\n",
    "        self.cd = Layer_CD()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ab(x)\n",
    "        x = self.cd(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "324b7a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function_A()\n",
      "------------------------------\n",
      "Function_B()\n",
      "------------------------------\n",
      "Layer_AB(\n",
      "  (a): Function_A()\n",
      "  (b): Function_B()\n",
      ")\n",
      "------------------------------\n",
      "Function_C()\n",
      "------------------------------\n",
      "Function_D()\n",
      "------------------------------\n",
      "Layer_CD(\n",
      "  (c): Function_C()\n",
      "  (d): Function_D()\n",
      ")\n",
      "------------------------------\n",
      "Model(\n",
      "  (ab): Layer_AB(\n",
      "    (a): Function_A()\n",
      "    (b): Function_B()\n",
      "  )\n",
      "  (cd): Layer_CD(\n",
      "    (c): Function_C()\n",
      "    (d): Function_D()\n",
      "  )\n",
      ")\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "def print_module(module):\n",
    "    print(module)\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# 🦆 apply는 apply가 적용된 module을 return 해줘요!\n",
    "returned_module = model.apply(print_module)\n",
    "# returned_module은 어디다 씀?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d3be8175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('ab.a.W', tensor([0.1178])), ('ab.b.W', tensor([0.3057])), ('cd.c.W', tensor([0.1448])), ('cd.d.W', tensor([0.4942]))])\n",
      "OrderedDict([('ab.a.W', tensor([1.])), ('ab.b.W', tensor([1.])), ('cd.c.W', tensor([1.])), ('cd.d.W', tensor([1.]))])\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "\n",
    "print(model.state_dict())\n",
    "\n",
    "# TODO : apply를 이용해 모든 Parameter 값을 1로 만들어보세요!\n",
    "def weight_initialization(module):\n",
    "    module_name = module.__class__.__name__\n",
    "\n",
    "    if module_name.split('_')[0] == \"Function\":\n",
    "        module.W.data.fill_(1.)\n",
    "\n",
    "# 🦆 apply는 apply가 적용된 module을 return 해줘요!\n",
    "returned_module = model.apply(weight_initialization)\n",
    "\n",
    "\n",
    "# 아래 코드는 수정하실 필요가 없습니다!\n",
    "x = torch.rand(1)\n",
    "\n",
    "output = model(x)\n",
    "\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "459194e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "\n",
    "# TODO : apply를 이용해서 부덕이가 원하는대로 repr 출력을 수정해주세요!\n",
    "from functools import partial\n",
    "\n",
    "def function_repr(self):\n",
    "    return f'name={self.name}'\n",
    "\n",
    "def add_repr(module):\n",
    "    module_name = module.__class__.__name__\n",
    "\n",
    "    if module_name.split('_')[0] == \"Function\":\n",
    "        module.extra_repr = partial(function_repr, module) # ???\n",
    "\n",
    "\n",
    "# 🦆 apply는 apply가 적용된 module을 return 해줘요!\n",
    "returned_module = model.apply(add_repr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ee8063cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (ab): Layer_AB(\n",
       "    (a): Function_A(name=plus)\n",
       "    (b): Function_B(name=substract)\n",
       "  )\n",
       "  (cd): Layer_CD(\n",
       "    (c): Function_C(name=multiply)\n",
       "    (d): Function_D(name=divide)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2a61b8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (ab): Layer_AB(\n",
      "    (a): Function_A(name=plus)\n",
      "    (b): Function_B(name=substract)\n",
      "  )\n",
      "  (cd): Layer_CD(\n",
      "    (c): Function_C(name=multiply)\n",
      "    (d): Function_D(name=divide)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_repr = repr(model)\n",
    "print(model_repr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfefa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재 4개의 Function A, B, C, D가 있어요!\n",
    "\n",
    "# - A : x + W\n",
    "# - B : x - W\n",
    "# - C : x * W\n",
    "# - D : x / W\n",
    "\n",
    "# 이걸 다음처럼 linear transformation처럼 동작하도록 바꿔보래요!\n",
    "\n",
    "# - A : x @ W + b\n",
    "# - B : x @ W + b\n",
    "# - C : x @ W + b\n",
    "# - D : x @ W + b\n",
    "\n",
    "# W는 이미 각 Function에 생성된 Parameter이고\n",
    "# b는 새롭게 만들어야 하는 Parameter에요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0eb84c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "\n",
    "# 아래 코드는 수정하실 필요가 없습니다!\n",
    "# 실행만 시켜주시고 다음 셀로 넘어가주세요!\n",
    "\n",
    "# Function\n",
    "class Function_A(nn.Module):\n",
    "    def __init__(self, name):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.W = Parameter(torch.rand(2, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.W\n",
    "\n",
    "class Function_B(nn.Module):\n",
    "    def __init__(self, name):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.W = Parameter(torch.rand(2, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x - self.W\n",
    "\n",
    "class Function_C(nn.Module):\n",
    "    def __init__(self, name):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.W = Parameter(torch.rand(2, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.W\n",
    "\n",
    "class Function_D(nn.Module):\n",
    "    def __init__(self, name):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.W = Parameter(torch.rand(2, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x / self.W\n",
    "\n",
    "\n",
    "# Layer\n",
    "class Layer_AB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.a = Function_A('plus')\n",
    "        self.b = Function_B('substract')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.a(x)\n",
    "        x = self.b(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Layer_CD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.c = Function_C('multiply')\n",
    "        self.d = Function_D('divide')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c(x)\n",
    "        x = self.d(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ab = Layer_AB()\n",
    "        self.cd = Layer_CD()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ab(x)\n",
    "        x = self.cd(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f76937bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# register_parameter()\n",
    "# data.fill_()\n",
    "# Function_A, B, C, D의 계산 결과 되돌리기\n",
    "# torch.addmm(b, output, W.T)\n",
    "# module.register.forward_hook()\n",
    "# returned_module = model.apply()\n",
    "\n",
    "model = Model()\n",
    "\n",
    "\n",
    "# TODO : apply를 이용해 Parameter b를 추가해보세요!\n",
    "def add_bias(module):\n",
    "    module_name = module.__class__.__name__\n",
    "\n",
    "    if module_name.split('_')[0] == \"Function\":\n",
    "        module.register_parameter('b', Parameter(torch.rand(2)))\n",
    "\n",
    "\n",
    "# TODO : apply를 이용해 추가된 b도 값을 1로 초기화해주세요!\n",
    "def weight_initialization(module):\n",
    "    module_name = module.__class__.__name__\n",
    "\n",
    "    if module_name.split('_')[0] == \"Function\":\n",
    "        module.W.data.fill_(1.)\n",
    "        module.b.data.fill_(1.)\n",
    "\n",
    "\n",
    "# TODO : apply를 이용해 모든 Function을 linear transformation으로 바꿔보세요!\n",
    "#        X @ W + b\n",
    "def linear_transformation(module):\n",
    "    module_name = module.__class__.__name__\n",
    "\n",
    "    if module_name == \"Function_A\":\n",
    "        def hook_A(module, input, output):\n",
    "            W, b = module.W, module.b\n",
    "            output = output - W \n",
    "            output = torch.addmm(b, output, W.T)\n",
    "\n",
    "            return output\n",
    "\n",
    "        module.register_forward_hook(hook_A)\n",
    "\n",
    "    elif module_name == \"Function_B\":\n",
    "        def hook_B(module, input, output):\n",
    "            W, b = module.W, module.b\n",
    "            output = output + W\n",
    "            output = torch.addmm(b, output, W.T)\n",
    "\n",
    "            return output\n",
    "\n",
    "        module.register_forward_hook(hook_B)\n",
    "\n",
    "    elif module_name == \"Function_C\":\n",
    "        def hook_C(module, input, output):\n",
    "            W, b = module.W, module.b\n",
    "            output = output / W\n",
    "            output = torch.addmm(b, output, W.T)\n",
    "\n",
    "            return output\n",
    "\n",
    "        module.register_forward_hook(hook_C)\n",
    "\n",
    "    elif module_name == \"Function_D\":\n",
    "        def hook_D(module, input, output):\n",
    "            W, b = module.W, module.b\n",
    "            output = output * W\n",
    "            output = torch.addmm(b, output, W.T)\n",
    "\n",
    "            return output\n",
    "\n",
    "        module.register_forward_hook(hook_D)\n",
    "\n",
    "returned_module = model.apply(add_bias)\n",
    "returned_module = model.apply(weight_initialization)\n",
    "returned_module = model.apply(linear_transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6aaf89e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('ab.a.W',\n",
       "              tensor([[1., 1.],\n",
       "                      [1., 1.]])),\n",
       "             ('ab.a.b', tensor([1., 1.])),\n",
       "             ('ab.b.W',\n",
       "              tensor([[1., 1.],\n",
       "                      [1., 1.]])),\n",
       "             ('ab.b.b', tensor([1., 1.])),\n",
       "             ('cd.c.W',\n",
       "              tensor([[1., 1.],\n",
       "                      [1., 1.]])),\n",
       "             ('cd.c.b', tensor([1., 1.])),\n",
       "             ('cd.d.W',\n",
       "              tensor([[1., 1.],\n",
       "                      [1., 1.]])),\n",
       "             ('cd.d.b', tensor([1., 1.]))])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "368b9551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7199, 0.8792],\n",
      "        [0.0292, 0.5850]], requires_grad=True)\n",
      "tensor(95.4112, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 2, requires_grad=True)\n",
    "\n",
    "print(x)\n",
    "\n",
    "output = model(x)\n",
    "output = output.sum()\n",
    "output.backward()\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b9fe5265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ab.a.W', tensor([[69.7755, 71.5895],\n",
      "        [69.7755, 71.5895]])), ('ab.a.b', tensor([144., 144.])), ('ab.b.W', tensor([[142.6825, 142.6825],\n",
      "        [142.6825, 142.6825]])), ('ab.b.b', tensor([72., 72.])), ('cd.c.W', tensor([[178.6825, 178.6825],\n",
      "        [178.6825, 178.6825]])), ('cd.c.b', tensor([36., 36.])), ('cd.d.W', tensor([[196.6825, 196.6825],\n",
      "        [196.6825, 196.6825]])), ('cd.d.b', tensor([18., 18.]))]\n"
     ]
    }
   ],
   "source": [
    "grads = [(name, param.grad) for name, param in model.named_parameters()]\n",
    "\n",
    "print(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a670f73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
